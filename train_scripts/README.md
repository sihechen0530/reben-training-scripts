# SLURM Training Infrastructure

è¿™ä¸ªç›®å½•åŒ…å«äº†ç”¨äºåœ¨SLURMé›†ç¾¤ä¸Šæäº¤BigEarthNetè®­ç»ƒä»»åŠ¡çš„åŸºç¡€è®¾æ–½ä»£ç ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

- **`config.yaml`** - ä¸»é…ç½®æ–‡ä»¶ï¼ŒåŒ…å«ä½œä¸šå‚æ•°ã€ç¯å¢ƒå˜é‡ã€æ•°æ®è·¯å¾„å’Œè®­ç»ƒå‚æ•°
- **`submit.py`** - Pythonæäº¤è„šæœ¬ï¼Œè¯»å–é…ç½®å¹¶æäº¤SLURMä½œä¸š
- **`sbatch_train.sbatch`** - SLURMæ‰¹å¤„ç†è„šæœ¬æ¨¡æ¿
- **`env_setup.sh`** - ç¯å¢ƒè®¾ç½®è„šæœ¬ï¼ˆåŠ è½½CUDAã€condaç­‰ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é…ç½®ç¯å¢ƒ

ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼Œè®¾ç½®ä½ çš„é›†ç¾¤å‚æ•°ï¼š

```yaml
job:
  partition: gpu          # ä½ çš„GPUåˆ†åŒºå
  account: your_account   # ä½ çš„è´¦å·
  chdir: "/path/to/reben-training-scripts"
  mail_user: "your@email.com"

data:
  benv2_data_dir: "/path/to/BigEarthNet/data"

train:
  args:
    architecture: "resnet18"
    batch_size: 32
    epochs: 100
    # ... å…¶ä»–è®­ç»ƒå‚æ•°
```

### 2. æäº¤å•ä¸ªè®­ç»ƒä»»åŠ¡

```bash
cd train_scripts
python submit.py
```

### 3. æäº¤è¶…å‚æ•°æœç´¢ä»»åŠ¡

åœ¨ `config.yaml` ä¸­é…ç½®sweepç½‘æ ¼ï¼š

```yaml
train:
  sweep:
    grid:
      lr: [0.001, 0.0003, 0.0001]
      batch_size: [32, 64, 128]
      seed: [42, 123, 456]
```

ç„¶åæäº¤ï¼š

```bash
python submit.py --sweep
```

è¿™ä¼šè‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆï¼ˆç¬›å¡å°”ç§¯ï¼‰å¹¶æäº¤ç‹¬ç«‹çš„ä½œä¸šã€‚

### 4. é¢„è§ˆä¸æäº¤ï¼ˆDry Runï¼‰

```bash
# é¢„è§ˆå•ä¸ªä½œä¸š
python submit.py --dry-run

# é¢„è§ˆsweepä½œä¸š
python submit.py --sweep --dry-run
```

## ğŸ“‹ é…ç½®æ–‡ä»¶è¯¦è§£

### Jobé…ç½®
```yaml
job:
  name: bigearthnet-ft     # ä½œä¸šåç§°
  partition: gpu           # SLURMåˆ†åŒº
  qos: normal             # QoSé˜Ÿåˆ—
  time: "08:00:00"        # æœ€å¤§è¿è¡Œæ—¶é—´
  nodes: 1                # èŠ‚ç‚¹æ•°
  gpus_per_task: 1        # æ¯ä¸ªä»»åŠ¡çš„GPUæ•°
  cpus_per_task: 8        # æ¯ä¸ªä»»åŠ¡çš„CPUæ ¸å¿ƒæ•°
  mem: "32G"              # å†…å­˜
  constraint: "a100|v100" # GPUå‹å·é™åˆ¶
```

### æ•°æ®é…ç½®
```yaml
data:
  benv2_data_dir: "/path/to/data"  # BigEarthNet v2.0 æ•°æ®ç›®å½•
```

### è®­ç»ƒå‚æ•°
```yaml
train:
  args:
    architecture: "resnet18"    # æ¨¡å‹æ¶æ„
    bandconfig: "all"           # æ³¢æ®µé…ç½®ï¼šall, s2, s1, rgb
    batch_size: 32              # æ‰¹å¤§å°
    epochs: 100                 # è®­ç»ƒè½®æ•°
    lr: 0.001                   # å­¦ä¹ ç‡
    seed: 42                    # éšæœºç§å­
    use_wandb: false            # æ˜¯å¦ä½¿ç”¨W&B
    config: "../train_scripts/config.yaml"  # æŒ‡å‘æ­¤é…ç½®æ–‡ä»¶
```

### è¶…å‚æ•°æœç´¢

**æ–¹å¼1: ç½‘æ ¼æœç´¢ï¼ˆç¬›å¡å°”ç§¯ï¼‰**
```yaml
sweep:
  grid:
    lr: [0.001, 0.0003]
    batch_size: [32, 64]
    seed: [42, 123]
```
è¿™ä¼šç”Ÿæˆ 2Ã—2Ã—2 = 8 ä¸ªä½œä¸š

**æ–¹å¼2: åˆ—è¡¨æ–‡ä»¶**
```yaml
sweep:
  list_file: "sweeps.txt"
```

`sweeps.txt` å†…å®¹ç¤ºä¾‹ï¼š
```yaml
{lr: 0.001, batch_size: 32, seed: 42}
{lr: 0.0003, batch_size: 64, seed: 123}
{lr: 0.0001, batch_size: 128, seed: 456}
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### ä½¿ç”¨DINOv3æ¨¡å‹

```yaml
train:
  args:
    architecture: "dinov3-base"
    linear_probe: true  # å†»ç»“backboneï¼Œåªè®­ç»ƒåˆ†ç±»å¤´
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤

```yaml
train:
  args:
    resume_from: "best"  # æˆ– "last" æˆ– "/path/to/checkpoint.ckpt"
```

### ä¸Šä¼ åˆ°HuggingFace Hub

```yaml
train:
  args:
    upload_to_hub: true
    hf_entity: "your-hf-username"
    use_wandb: true
```

### ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶

```bash
python submit.py --config my_config.yaml
python submit.py --config my_config.yaml --sweep
```

### ä½¿ç”¨è‡ªå®šä¹‰sbatchæ¨¡æ¿

```bash
python submit.py --template my_template.sbatch
```

### å¤šæ¨¡æ€è®­ç»ƒï¼ˆMultimodal Trainingï¼‰

å¤šæ¨¡æ€è®­ç»ƒæ”¯æŒå¤šä¸ªbackboneã€å¤šç§èåˆç­–ç•¥å’Œåˆ†ç±»å™¨ç±»å‹ã€‚è¦ä½¿ç”¨å¤šæ¨¡æ€è®­ç»ƒï¼š

1. **åˆ‡æ¢åˆ°å¤šæ¨¡æ€è®­ç»ƒè„šæœ¬**ï¼š
```yaml
train:
  script: "train_multimodal.py"  # ä» train_BigEarthNetv2_0.py æ”¹ä¸º train_multimodal.py
```

2. **é…ç½®å¤šæ¨¡æ€å‚æ•°**ï¼ˆåœ¨ `multimodal_args` éƒ¨åˆ†ï¼‰ï¼š
```yaml
train:
  script: "train_multimodal.py"
  multimodal_args:
    # åŸºç¡€å‚æ•°
    seed: 42
    lr: 0.001
    epochs: 100
    bs: 512
    
    # DINOv3 backboneï¼ˆå¤„ç†RGBï¼Œ3é€šé“ï¼‰
    dinov3_hidden_size: 768      # 384 (small), 768 (base), 1024 (large), 1536 (giant)
    dinov3_pretrained: true
    dinov3_freeze: false         # true = å†»ç»“ï¼ˆçº¿æ€§æ¢æµ‹ï¼‰, false = å¾®è°ƒ
    dinov3_lr: 0.0001
    
    # ResNet101 backboneï¼ˆå¤„ç†S2éRGB + å¯é€‰S1ï¼‰
    resnet_pretrained: true
    resnet_freeze: false         # true = å†»ç»“, false = å¾®è°ƒ
    resnet_lr: 0.0001
    
    # èåˆç­–ç•¥
    fusion_type: "concat"        # concat (é»˜è®¤), weighted, linear_projection
    # fusion_output_dim: 512     # ä»…ç”¨äº linear_projection
    
    # åˆ†ç±»å™¨
    classifier_type: "linear"    # linear (é»˜è®¤ï¼Œçº¿æ€§æ¢æµ‹), mlp
    classifier_hidden_dim: 512  # ä»…ç”¨äº MLP
    
    # æ•°æ®é…ç½®
    use_s1: false               # true = åŒ…å«S1ï¼ˆ11é€šé“ï¼‰, false = ä»…S2éRGBï¼ˆ9é€šé“ï¼‰
```

3. **å¤šæ¨¡æ€è®­ç»ƒç¤ºä¾‹**ï¼š

**ç¤ºä¾‹1ï¼šçº¿æ€§æ¢æµ‹ï¼ˆå†»ç»“æ‰€æœ‰backboneï¼‰**
```yaml
train:
  script: "train_multimodal.py"
  multimodal_args:
    dinov3_freeze: true
    resnet_freeze: true
    fusion_type: "concat"
    classifier_type: "linear"
```

**ç¤ºä¾‹2ï¼šåŠ æƒèåˆ + MLPåˆ†ç±»å™¨**
```yaml
train:
  script: "train_multimodal.py"
  multimodal_args:
    fusion_type: "weighted"
    classifier_type: "mlp"
    classifier_hidden_dim: 512
```

**ç¤ºä¾‹3ï¼šåŒ…å«S1æ•°æ®**
```yaml
train:
  script: "train_multimodal.py"
  multimodal_args:
    use_s1: true  # ResNetå°†å¤„ç†11é€šé“ï¼ˆ9 S2éRGB + 2 S1ï¼‰
```

4. **å¤šæ¨¡æ€è¶…å‚æœç´¢**ï¼š
```yaml
train:
  script: "train_multimodal.py"
  multimodal_args:
    # ... åŸºç¡€é…ç½® ...
  sweep:
    grid:
      fusion_type: ["concat", "weighted", "linear_projection"]
      classifier_type: ["linear", "mlp"]
      dinov3_freeze: [true, false]
      resnet_freeze: [true, false]
```

### å¤šGPUè®­ç»ƒï¼ˆMulti-GPU Trainingï¼‰

PyTorch Lightningæ”¯æŒå¤šGPUè®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

#### é…ç½®å¤šGPUè®­ç»ƒ

**æ–¹å¼1ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰**

åœ¨ `config.yaml` ä¸­é…ç½®ï¼š

```yaml
job:
  gres: "gpu:v100-sxm2:4"  # è¯·æ±‚4ä¸ªGPU
  mem: "64G"                # å¤šGPUæ—¶å¢åŠ å†…å­˜

train:
  args:
    devices: 4              # ä½¿ç”¨4ä¸ªGPU
    strategy: "ddp"         # ä½¿ç”¨DDPç­–ç•¥ï¼ˆæ¨èï¼‰
    bs: 512                 # æ¯ä¸ªGPUçš„batch size
    # æ€»batch size = bs * num_gpus = 512 * 4 = 2048
```

**æ–¹å¼2ï¼šé€šè¿‡å‘½ä»¤è¡Œå‚æ•°**

```bash
python scripts/train_BigEarthNetv2_0.py \
    --devices 4 \
    --strategy ddp \
    --bs 512
```

#### å¤šGPUè®­ç»ƒç­–ç•¥

- **`ddp`** (æ¨è): DistributedDataParallelï¼Œå•èŠ‚ç‚¹å¤šGPUè®­ç»ƒçš„æœ€ä½³é€‰æ‹©
- **`ddp_spawn`**: DDP with spawnï¼Œé€‚ç”¨äºæŸäº›ç¯å¢ƒï¼ˆWindowsã€Jupyterï¼‰
- **`deepspeed`**: DeepSpeedç­–ç•¥ï¼Œéœ€è¦å®‰è£…DeepSpeedåº“
- **`fsdp`**: Fully Sharded Data Parallelï¼Œé€‚ç”¨äºå¤§æ¨¡å‹

#### é‡è¦æ³¨æ„äº‹é¡¹

1. **Batch Size**: 
   - é…ç½®çš„ `bs` æ˜¯æ¯ä¸ªGPUçš„batch size
   - æ€»batch size = `bs * num_gpus`
   - ä¾‹å¦‚ï¼š4ä¸ªGPUï¼Œ`bs=512` â†’ æ€»batch size = 2048

2. **å­¦ä¹ ç‡è°ƒæ•´**:
   - å¤šGPUè®­ç»ƒæ—¶ï¼Œé€šå¸¸éœ€è¦æŒ‰GPUæ•°é‡çº¿æ€§ç¼©æ”¾å­¦ä¹ ç‡
   - ä¾‹å¦‚ï¼šå•GPU `lr=0.001`ï¼Œ4ä¸ªGPUæ—¶å»ºè®® `lr=0.004`
   - æˆ–è€…ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨è‡ªåŠ¨è°ƒæ•´

3. **Workersæ•°é‡**:
   - å»ºè®® `workers = num_gpus * 2-4`
   - ä¾‹å¦‚ï¼š4ä¸ªGPU â†’ `workers=8` æˆ– `workers=16`

4. **å†…å­˜éœ€æ±‚**:
   - å¤šGPUè®­ç»ƒéœ€è¦æ›´å¤šç³»ç»Ÿå†…å­˜
   - å»ºè®®ï¼š`mem = "32G" * num_gpus`ï¼ˆè‡³å°‘ï¼‰

5. **SLURMé…ç½®**:
   ```yaml
   job:
     gres: "gpu:v100-sxm2:4"  # è¯·æ±‚4ä¸ªGPU
     nodes: 1                  # å•èŠ‚ç‚¹å¤šGPU
     cpus_per_task: 16         # å¢åŠ CPUæ ¸å¿ƒæ•°
     mem: "64G"                # å¢åŠ å†…å­˜
   ```

#### å¤šGPUè®­ç»ƒç¤ºä¾‹

**ç¤ºä¾‹1ï¼š4ä¸ªGPUè®­ç»ƒ**
```yaml
job:
  gres: "gpu:v100-sxm2:4"
  mem: "64G"
  cpus_per_task: 16

train:
  args:
    devices: 4
    strategy: "ddp"
    bs: 512
    lr: 0.004  # çº¿æ€§ç¼©æ”¾ï¼š0.001 * 4
    workers: 16
```

**ç¤ºä¾‹2ï¼š8ä¸ªGPUè®­ç»ƒ**
```yaml
job:
  gres: "gpu:a100:8"
  mem: "128G"
  cpus_per_task: 32

train:
  args:
    devices: 8
    strategy: "ddp"
    bs: 256   # æ¯ä¸ªGPUçš„batch size
    lr: 0.008  # çº¿æ€§ç¼©æ”¾ï¼š0.001 * 8
    workers: 32
```

#### éªŒè¯å¤šGPUè®­ç»ƒ

è®­ç»ƒå¼€å§‹æ—¶ä¼šçœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š
```
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
```

#### æ•…éšœæ’æŸ¥

1. **CUDA out of memory**: 
   - å‡å°‘ `bs`ï¼ˆæ¯ä¸ªGPUçš„batch sizeï¼‰
   - å¢åŠ  `mem`ï¼ˆç³»ç»Ÿå†…å­˜ï¼‰

2. **NCCLé”™è¯¯**:
   - ç¡®ä¿æ‰€æœ‰GPUåœ¨åŒä¸€èŠ‚ç‚¹ä¸Š
   - æ£€æŸ¥ç½‘ç»œé…ç½®ï¼ˆInfiniBandç­‰ï¼‰

3. **è®­ç»ƒé€Ÿåº¦æ²¡æœ‰æå‡**:
   - æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æˆä¸ºç“¶é¢ˆï¼ˆå¢åŠ  `workers`ï¼‰
   - ç¡®ä¿batch sizeè¶³å¤Ÿå¤§
   - æ£€æŸ¥GPUåˆ©ç”¨ç‡ï¼ˆ`nvidia-smi`ï¼‰

## ğŸ“Š ç›‘æ§ä½œä¸š

```bash
# æŸ¥çœ‹ä½œä¸šé˜Ÿåˆ—
squeue -u $USER

# æŸ¥çœ‹ç‰¹å®šä½œä¸š
squeue -j JOB_ID

# å–æ¶ˆä½œä¸š
scancel JOB_ID

# æŸ¥çœ‹ä½œä¸šè¾“å‡º
tail -f logs/bigearthnet-ft-JOBID.out
```

## ğŸ› æ•…éšœæ’æŸ¥

### ä½œä¸šå¤±è´¥
1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ï¼š`logs/bigearthnet-ft-JOBID.err`
2. éªŒè¯æ•°æ®è·¯å¾„ï¼šç¡®ä¿ `benv2_data_dir` æŒ‡å‘æ­£ç¡®çš„æ•°æ®é›†
3. æ£€æŸ¥ç¯å¢ƒï¼šç¡®ä¿ `env_setup.sh` æ­£ç¡®åŠ è½½äº†condaç¯å¢ƒ

### æ•°æ®è·¯å¾„é—®é¢˜
è®­ç»ƒè„šæœ¬ä¼šæŒ‰ä»¥ä¸‹é¡ºåºæŸ¥æ‰¾æ•°æ®ç›®å½•ï¼š
1. å¦‚æœæä¾›äº† `--config` å‚æ•°ï¼Œä»é…ç½®æ–‡ä»¶è¯»å– `data.benv2_data_dir`
2. å¦åˆ™ï¼Œæ ¹æ®hostnameè‡ªåŠ¨é€‰æ‹©ï¼ˆmars, erde, plutoç­‰ï¼‰
3. æœ€åå›é€€åˆ°é»˜è®¤è·¯å¾„

### æäº¤è„šæœ¬æ‰¾ä¸åˆ°sbatch
å¦‚æœçœ‹åˆ° "sbatch command not found"ï¼Œç¡®ä¿ï¼š
1. ä½ åœ¨SLURMé›†ç¾¤ä¸Šè¿è¡Œ
2. SLURMæ¨¡å—å·²åŠ è½½ï¼ˆå¯èƒ½éœ€è¦ `module load slurm`ï¼‰

## ğŸ“š ç¤ºä¾‹å·¥ä½œæµ

### å•æ¬¡å®éªŒ
```bash
# 1. ç¼–è¾‘config.yamlè®¾ç½®å‚æ•°
vim config.yaml

# 2. é¢„è§ˆ
python submit.py --dry-run

# 3. æäº¤
python submit.py

# 4. ç›‘æ§
squeue -u $USER
tail -f logs/bigearthnet-ft-*.out
```

### è¶…å‚æ•°æœç´¢
```bash
# 1. é…ç½®sweepç½‘æ ¼
vim config.yaml  # ç¼–è¾‘ train.sweep.grid

# 2. é¢„è§ˆæ‰€æœ‰ä½œä¸š
python submit.py --sweep --dry-run

# 3. æäº¤å…¨éƒ¨
python submit.py --sweep

# 4. ç›‘æ§å…¨éƒ¨ä½œä¸š
watch -n 10 squeue -u $USER
```

## ğŸ”— ç›¸å…³æ–‡ä»¶

- è®­ç»ƒè„šæœ¬: `../scripts/train_BigEarthNetv2_0.py`
- å·¥å…·å‡½æ•°: `../scripts/utils.py`
- é¡¹ç›®README: `../README.md`
